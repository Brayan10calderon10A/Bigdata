# -*- coding: utf-8 -*-
"""Modelo_ml_spark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SJMmygC6IkCpq_6pYqW666FWvZ8pJgmD
"""

!pip install pyspark

import pyspark
from pyspark import SparkContext
from pyspark.sql import SparkSession, SQLContext

import pandas as pd
import warnings
import re
import matplotlib.pyplot as plt

from pyspark.sql.functions import when, col, max, countDistinct, count, avg
import pyspark.sql.functions as F
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType,IntegerType, StructType, StructField

pd.set_option("display.max_columns", None)
warnings.filterwarnings("ignore")

from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression

spark = SparkSession.builder.appName("Modelos").getOrCreate()

access_key = 'AKIAVFTC65SN7KSWPHU3'
secret_key = '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'

# Configurar las credenciales si no se han configurado previamente
#spark.sparkContext._jsc.hadoopConfiguration().set("fs.s3a.access.key", access_key)
#spark.sparkContext._jsc.hadoopConfiguration().set("fs.s3a.secret.key", secret_key)

bucket_origen = 'archivos-curados'
bucket_destino = 'archivos-curados'

ruta_origen = 's3a://{}/frame_0.csv'.format(bucket_origen)
encuesta = spark.sparkContext.textFile(ruta_origen)

encuesta = spark.sparkContext.textFile("/content/Resultados_final.csv")

encuesta2 = encuesta.map(lambda x: x.split(";"))

encuesta2.take(1)

def interpolacion_aleatoria(row):
    x, y = row
    if y is None:
        valores_conocidos = encuesta2.filter(lambda r: r[1] is not None).map(lambda r: r[1]).collect()
        valor_interpolado = encuesta2.choice(valores_conocidos)
        return x, valor_interpolado
    else:
        return x, y

resultados_interpolados = encuesta2.map(interpolacion_aleatoria)

def reemplazar_con_cero(valor):
    if valor is None or re.search(pattern = r"\s|\t|''", string = valor):
        return 0
    else:
        return valor

mi_rdd_sin_nulos_espacios = encuesta2.map(reemplazar_con_cero)

num_filas = encuesta2.count()

# Obtener el número de columnas
num_columnas = len(encuesta2.first())

# Calcular el porcentaje de valores nulos y vacíos por columna
porcentaje_nulos_vacios_por_columna = encuesta2.flatMap(lambda row: [(i, 1 if val is None else 0, 1 if val == '' else 0) for i, val in enumerate(row)]) \
                                            .map(lambda x: (x[0], (x[1], x[2]))) \
                                            .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \
                                            .mapValues(lambda x: (x[0] / num_filas * 100, x[1] / num_filas * 100)) \
                                            .collect()

# Mostrar el porcentaje de nulos y vacíos por columna
for columna, (porcentaje_nulos, porcentaje_vacios) in porcentaje_nulos_vacios_por_columna:
    print(f"Columna {columna}: Porcentaje de nulos: {porcentaje_nulos}%, Porcentaje de vacíos: {porcentaje_vacios}%")

from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import monotonically_increasing_id
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator

def convertir_a_float(row):
    return tuple(float(x) if isinstance(x, str) and x.replace('.', '', 1).isdigit() else x for x in row)

mi_rdd_sin_nulos_espacios = mi_rdd_sin_nulos_espacios.map(lambda x: (x[12],x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8],x[9],x[10].replace(",", "."),x[11],x[13],x[14],x[15],x[16],x[17],x[18],x[19],x[20],x[21],x[22].replace(",", ".")))

encuesta2 = encuesta2.map(lambda x: (x[12],x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8],x[9],x[10].replace(",", "."),x[11],x[13],x[14],x[15],x[16],x[17],x[18],x[19],x[20],x[21],x[22].replace(",", ".")))

header = encuesta2.first()
schema = StructType([StructField(header_, StringType(), True) for header_ in header])
datos_sin_header = encuesta2.filter(lambda row: row != header)
frame = spark.createDataFrame(datos_sin_header, schema)

datos_sin_header = datos_sin_header.map(convertir_a_float)

modelos_datos = encuesta2.map(lambda x: (x[0], (x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8],x[9],x[10],x[11],x[13],x[14],x[15],x[16],x[17],x[18],x[19],x[20],x[21],x[22])))

def cast_first_field_to_int(row):
    try:
        # Intenta convertir el primer campo a entero y devuelve una tupla con la conversión
        return (int(row[0]), row[1])
    except ValueError:
        # En caso de error de conversión, retorna la fila original
        return row

modified_rdd = modelos_datos.map(cast_first_field_to_int)

#rdd_rows = modified_rdd.map(lambda x: Row(label=x[0], features=x[1]))

#df = spark.createDataFrame(datos_sin_header)

frame.show()

#df = df.withColumn("row_id", monotonically_increasing_id())
#min_row_id = df.selectExpr("min(row_id)").first()[0]
#
## Filtrar el DataFrame para eliminar la primera fila
#df_sin_primera_fila = df.filter(df.row_id != min_row_id).drop("row_id")
#
## Mostrar el DataFrame resultante
#df_sin_primera_fila.show()

#df = df_sin_primera_fila.withColumnRenamed("label", "etiqueta")

frame = frame.withColumn("Sexo", col("Sexo").cast("int"))
frame = frame.withColumn("AnoNac", col("AnoNac").cast("double"))
frame = frame.withColumn("Relacion", col("Relacion").cast("double"))
frame = frame.withColumn("LicenciaConducir", col("LicenciaConducir").cast("double"))
frame = frame.withColumn("AdultoMayor", col("AdultoMayor").cast("double"))
frame = frame.withColumn("Curso", col("Curso").cast("double"))
frame = frame.withColumn("ConoceTransantiago", col("ConoceTransantiago").cast("double"))
frame = frame.withColumn("Discapacidad", col("Discapacidad").cast("double"))
frame = frame.withColumn("Ingreso", col("Ingreso").cast("double"))
frame = frame.withColumn("IngresoFinal", col("IngresoFinal").cast("double"))
frame = frame.withColumn("IngresoImputado", col("IngresoImputado").cast("double"))
frame = frame.withColumn("Factor", col("Factor").cast("double"))
frame = frame.withColumn("Persona", col("Persona").cast("double"))
frame = frame.withColumn("Viajes", col("Viajes").cast("double"))
frame = frame.withColumn("PaseEscolar", col("PaseEscolar").cast("double"))
frame = frame.withColumn("Estudios", col("Estudios").cast("double"))
frame = frame.withColumn("Actividad", col("Actividad").cast("double"))
frame = frame.withColumn("TarjetaBip", col("TarjetaBip").cast("double"))
frame = frame.withColumn("NoUsaTransantiago", col("NoUsaTransantiago").cast("double"))
frame = frame.withColumn("TieneIngresos", col("TieneIngresos").cast("double"))
frame = frame.withColumn("TramoIngreso", col("TramoIngreso").cast("double"))
frame = frame.withColumn("TramoIngresoFinal", col("TramoIngresoFinal").cast("double"))
frame = frame.withColumn("Factor_LaboralNormal", col("Factor_LaboralNormal").cast("double"))

vector_assembler = VectorAssembler(inputCols = ["AnoNac","Relacion","LicenciaConducir","AdultoMayor","Curso","ConoceTransantiago","Discapacidad","Ingreso","IngresoFinal","IngresoImputado","Factor","Persona","Viajes","PaseEscolar","Estudios","Actividad","TarjetaBip","NoUsaTransantiago","TieneIngresos","TramoIngreso","TramoIngresoFinal","Factor_LaboralNormal"],
                                   outputCol="features",
                                   handleInvalid = "keep")

vector_assembler.transform(frame).show()

data = vector_assembler.transform(frame)

data = data.withColumn("Sexo", when(data["Sexo"] == 2, 0).otherwise(data["Sexo"]))

# Dividir los datos en conjunto de entrenamiento y prueba
(train_data, test_data) = data.randomSplit([0.7, 0.3])

# Inicializar el clasificador Random Forest
rf = RandomForestClassifier(featuresCol="features", labelCol="Sexo")

#rf = rf.setParams(numTrees=200, maxDepth=20, maxBins=22)
# Entrenar el modelo
model = rf.fit(train_data)

# Realizar predicciones en el conjunto de prueba
predictions = model.transform(test_data)

predictions.show()

frame_resultado = predictions.select("Sexo", col("features").cast("string"), col("rawPrediction").cast("string"), col("probability").cast("string"),"prediction")

frame_resultado.write.csv("predicciones.csv")

# Evaluar el desempeño del modelo
evaluator = MulticlassClassificationEvaluator(labelCol="Sexo", predictionCol="prediction",
                                              metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Test Error = %g" % (1.0 - accuracy))

accuracy = evaluator.evaluate(predictions)
precision = evaluator.evaluate(predictions, {evaluator.metricName: "weightedPrecision"})
recall = evaluator.evaluate(predictions, {evaluator.metricName: "weightedRecall"})
f1_score = evaluator.evaluate(predictions, {evaluator.metricName: "f1"})

print("accuracy: ",accuracy)
print("precision: ",precision)
print("recall: ",recall)
print("f1_score: ",f1_score)

importances = model.featureImportances.toArray()

# Crear un gráfico de barras para mostrar la importancia de las características
feature_names = ['feature']  # Reemplaza con los nombres de tus características
plt.barh(feature_names, importances)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

from sklearn.metrics import roc_curve, auc

# Suponiendo que 'predictions' contiene las predicciones del modelo para clasificación binaria

# Obtener las tasas de falsos positivos y verdaderos positivos
y_prob = predictions.select('probability').rdd.map(lambda x: x[0][1])
y_true = predictions.select('Sexo').rdd.map(lambda row: row[0])

y_true.collect()

fpr, tpr, thresholds = roc_curve(y_true.collect(), y_prob.collect())

roc_curve(y_true.collect(), y_prob.collect())

# Calcular el área bajo la curva ROC (AUC)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()